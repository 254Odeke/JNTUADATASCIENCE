{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine:\n",
    "\n",
    "* This is one of the classifier.\n",
    "* This is can be used as classifier as well as regressor.\n",
    "* The classification is done using the plane \n",
    "    * Two vectors used to classify the elements into categories\n",
    "    * first vector drawn nearer to the first category element and second vector drawn nearer to the second category element.\n",
    "    * The middle line of the two vectors called as maximum margin hyperplane.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"data:image/png;base64,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\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv(\"https://raw.githubusercontent.com/AP-Skill-Development-Corporation/Tirumala-ML/main/Day-6/winequality.csv\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           1599 non-null float64\n",
      "volatile acidity        1599 non-null float64\n",
      "citric acid             1599 non-null float64\n",
      "residual sugar          1599 non-null float64\n",
      "chlorides               1599 non-null float64\n",
      "free sulfur dioxide     1599 non-null float64\n",
      "total sulfur dioxide    1599 non-null float64\n",
      "density                 1599 non-null float64\n",
      "pH                      1599 non-null float64\n",
      "sulphates               1599 non-null float64\n",
      "alcohol                 1599 non-null float64\n",
      "quality                 1599 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20779fab898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Quality vs sulphates:\n",
    "\n",
    "sns.barplot(x = 'quality',y = \"sulphates\",data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## quality and volatile acidity:\n",
    "\n",
    "sns.barplot(x = \"quality\",y = \"volatile acidity\",data = d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [2,6.5,8]\n",
    "group_name = [\"bad\",\"good\"]\n",
    "categories = pd.cut(d['quality'],bins,labels = group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['quality'] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        bad\n",
       "1        bad\n",
       "2        bad\n",
       "3        bad\n",
       "4        bad\n",
       "5        bad\n",
       "6        bad\n",
       "7       good\n",
       "8       good\n",
       "9        bad\n",
       "10       bad\n",
       "11       bad\n",
       "12       bad\n",
       "13       bad\n",
       "14       bad\n",
       "15       bad\n",
       "16      good\n",
       "17       bad\n",
       "18       bad\n",
       "19       bad\n",
       "20       bad\n",
       "21       bad\n",
       "22       bad\n",
       "23       bad\n",
       "24       bad\n",
       "25       bad\n",
       "26       bad\n",
       "27       bad\n",
       "28       bad\n",
       "29       bad\n",
       "        ... \n",
       "1569     bad\n",
       "1570     bad\n",
       "1571     bad\n",
       "1572     bad\n",
       "1573     bad\n",
       "1574     bad\n",
       "1575     bad\n",
       "1576     bad\n",
       "1577     bad\n",
       "1578     bad\n",
       "1579     bad\n",
       "1580     bad\n",
       "1581     bad\n",
       "1582     bad\n",
       "1583     bad\n",
       "1584    good\n",
       "1585     bad\n",
       "1586     bad\n",
       "1587     bad\n",
       "1588     bad\n",
       "1589     bad\n",
       "1590     bad\n",
       "1591     bad\n",
       "1592     bad\n",
       "1593     bad\n",
       "1594     bad\n",
       "1595     bad\n",
       "1596     bad\n",
       "1597     bad\n",
       "1598     bad\n",
       "Name: quality, Length: 1599, dtype: category\n",
       "Categories (2, object): [bad < good]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad     1382\n",
       "good     217\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2077a3c5d68>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x = 'quality',y = 'alcohol',data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d.drop(['quality'],axis = 1)  ### input\n",
    "y = d['quality']   ### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.097</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99430</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.114</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.176</td>\n",
       "      <td>52.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.170</td>\n",
       "      <td>51.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.092</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.28</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.341</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.082</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.084</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.080</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.082</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.056</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99396</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99340</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.93</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.038</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99514</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.65</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.069</td>\n",
       "      <td>35.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99632</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.075</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.99467</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.67</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.78</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.074</td>\n",
       "      <td>23.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99677</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.060</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99474</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.64</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.081</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99588</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.78</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.076</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.118</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.068</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.053</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99362</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.074</td>\n",
       "      <td>32.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99578</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.061</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99484</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.066</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.065</td>\n",
       "      <td>34.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.066</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.99483</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.068</td>\n",
       "      <td>34.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99414</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.073</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99770</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99314</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.82</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "5               7.4             0.660         0.00             1.8      0.075   \n",
       "6               7.9             0.600         0.06             1.6      0.069   \n",
       "7               7.3             0.650         0.00             1.2      0.065   \n",
       "8               7.8             0.580         0.02             2.0      0.073   \n",
       "9               7.5             0.500         0.36             6.1      0.071   \n",
       "10              6.7             0.580         0.08             1.8      0.097   \n",
       "11              7.5             0.500         0.36             6.1      0.071   \n",
       "12              5.6             0.615         0.00             1.6      0.089   \n",
       "13              7.8             0.610         0.29             1.6      0.114   \n",
       "14              8.9             0.620         0.18             3.8      0.176   \n",
       "15              8.9             0.620         0.19             3.9      0.170   \n",
       "16              8.5             0.280         0.56             1.8      0.092   \n",
       "17              8.1             0.560         0.28             1.7      0.368   \n",
       "18              7.4             0.590         0.08             4.4      0.086   \n",
       "19              7.9             0.320         0.51             1.8      0.341   \n",
       "20              8.9             0.220         0.48             1.8      0.077   \n",
       "21              7.6             0.390         0.31             2.3      0.082   \n",
       "22              7.9             0.430         0.21             1.6      0.106   \n",
       "23              8.5             0.490         0.11             2.3      0.084   \n",
       "24              6.9             0.400         0.14             2.4      0.085   \n",
       "25              6.3             0.390         0.16             1.4      0.080   \n",
       "26              7.6             0.410         0.24             1.8      0.080   \n",
       "27              7.9             0.430         0.21             1.6      0.106   \n",
       "28              7.1             0.710         0.00             1.9      0.080   \n",
       "29              7.8             0.645         0.00             2.0      0.082   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1569            6.2             0.510         0.14             1.9      0.056   \n",
       "1570            6.4             0.360         0.53             2.2      0.230   \n",
       "1571            6.4             0.380         0.14             2.2      0.038   \n",
       "1572            7.3             0.690         0.32             2.2      0.069   \n",
       "1573            6.0             0.580         0.20             2.4      0.075   \n",
       "1574            5.6             0.310         0.78            13.9      0.074   \n",
       "1575            7.5             0.520         0.40             2.2      0.060   \n",
       "1576            8.0             0.300         0.63             1.6      0.081   \n",
       "1577            6.2             0.700         0.15             5.1      0.076   \n",
       "1578            6.8             0.670         0.15             1.8      0.118   \n",
       "1579            6.2             0.560         0.09             1.7      0.053   \n",
       "1580            7.4             0.350         0.33             2.4      0.068   \n",
       "1581            6.2             0.560         0.09             1.7      0.053   \n",
       "1582            6.1             0.715         0.10             2.6      0.053   \n",
       "1583            6.2             0.460         0.29             2.1      0.074   \n",
       "1584            6.7             0.320         0.44             2.4      0.061   \n",
       "1585            7.2             0.390         0.44             2.6      0.066   \n",
       "1586            7.5             0.310         0.41             2.4      0.065   \n",
       "1587            5.8             0.610         0.11             1.8      0.066   \n",
       "1588            7.2             0.660         0.33             2.5      0.068   \n",
       "1589            6.6             0.725         0.20             7.8      0.073   \n",
       "1590            6.3             0.550         0.15             1.8      0.077   \n",
       "1591            5.4             0.740         0.09             1.7      0.089   \n",
       "1592            6.3             0.510         0.13             2.3      0.076   \n",
       "1593            6.8             0.620         0.08             1.9      0.068   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "6                    15.0                  59.0  0.99640  3.30       0.46   \n",
       "7                    15.0                  21.0  0.99460  3.39       0.47   \n",
       "8                     9.0                  18.0  0.99680  3.36       0.57   \n",
       "9                    17.0                 102.0  0.99780  3.35       0.80   \n",
       "10                   15.0                  65.0  0.99590  3.28       0.54   \n",
       "11                   17.0                 102.0  0.99780  3.35       0.80   \n",
       "12                   16.0                  59.0  0.99430  3.58       0.52   \n",
       "13                    9.0                  29.0  0.99740  3.26       1.56   \n",
       "14                   52.0                 145.0  0.99860  3.16       0.88   \n",
       "15                   51.0                 148.0  0.99860  3.17       0.93   \n",
       "16                   35.0                 103.0  0.99690  3.30       0.75   \n",
       "17                   16.0                  56.0  0.99680  3.11       1.28   \n",
       "18                    6.0                  29.0  0.99740  3.38       0.50   \n",
       "19                   17.0                  56.0  0.99690  3.04       1.08   \n",
       "20                   29.0                  60.0  0.99680  3.39       0.53   \n",
       "21                   23.0                  71.0  0.99820  3.52       0.65   \n",
       "22                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "23                    9.0                  67.0  0.99680  3.17       0.53   \n",
       "24                   21.0                  40.0  0.99680  3.43       0.63   \n",
       "25                   11.0                  23.0  0.99550  3.34       0.56   \n",
       "26                    4.0                  11.0  0.99620  3.28       0.59   \n",
       "27                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "28                   14.0                  35.0  0.99720  3.47       0.55   \n",
       "29                    8.0                  16.0  0.99640  3.38       0.59   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1569                 15.0                  34.0  0.99396  3.48       0.57   \n",
       "1570                 19.0                  35.0  0.99340  3.37       0.93   \n",
       "1571                 15.0                  25.0  0.99514  3.44       0.65   \n",
       "1572                 35.0                 104.0  0.99632  3.33       0.51   \n",
       "1573                 15.0                  50.0  0.99467  3.58       0.67   \n",
       "1574                 23.0                  92.0  0.99677  3.39       0.48   \n",
       "1575                 12.0                  20.0  0.99474  3.26       0.64   \n",
       "1576                 16.0                  29.0  0.99588  3.30       0.78   \n",
       "1577                 13.0                  27.0  0.99622  3.54       0.60   \n",
       "1578                 13.0                  20.0  0.99540  3.42       0.67   \n",
       "1579                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1580                  9.0                  26.0  0.99470  3.36       0.60   \n",
       "1581                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1582                 13.0                  27.0  0.99362  3.57       0.50   \n",
       "1583                 32.0                  98.0  0.99578  3.33       0.62   \n",
       "1584                 24.0                  34.0  0.99484  3.29       0.80   \n",
       "1585                 22.0                  48.0  0.99494  3.30       0.84   \n",
       "1586                 34.0                  60.0  0.99492  3.34       0.85   \n",
       "1587                 18.0                  28.0  0.99483  3.55       0.66   \n",
       "1588                 34.0                 102.0  0.99414  3.27       0.78   \n",
       "1589                 29.0                  79.0  0.99770  3.29       0.54   \n",
       "1590                 26.0                  35.0  0.99314  3.32       0.82   \n",
       "1591                 16.0                  26.0  0.99402  3.67       0.56   \n",
       "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  \n",
       "0         9.4  \n",
       "1         9.8  \n",
       "2         9.8  \n",
       "3         9.8  \n",
       "4         9.4  \n",
       "5         9.4  \n",
       "6         9.4  \n",
       "7        10.0  \n",
       "8         9.5  \n",
       "9        10.5  \n",
       "10        9.2  \n",
       "11       10.5  \n",
       "12        9.9  \n",
       "13        9.1  \n",
       "14        9.2  \n",
       "15        9.2  \n",
       "16       10.5  \n",
       "17        9.3  \n",
       "18        9.0  \n",
       "19        9.2  \n",
       "20        9.4  \n",
       "21        9.7  \n",
       "22        9.5  \n",
       "23        9.4  \n",
       "24        9.7  \n",
       "25        9.3  \n",
       "26        9.5  \n",
       "27        9.5  \n",
       "28        9.4  \n",
       "29        9.8  \n",
       "...       ...  \n",
       "1569     11.5  \n",
       "1570     12.4  \n",
       "1571     11.1  \n",
       "1572      9.5  \n",
       "1573     12.5  \n",
       "1574     10.5  \n",
       "1575     11.8  \n",
       "1576     10.8  \n",
       "1577     11.9  \n",
       "1578     11.3  \n",
       "1579     11.3  \n",
       "1580     11.9  \n",
       "1581     11.3  \n",
       "1582     11.9  \n",
       "1583      9.8  \n",
       "1584     11.6  \n",
       "1585     11.5  \n",
       "1586     11.4  \n",
       "1587     10.9  \n",
       "1588     12.8  \n",
       "1589      9.2  \n",
       "1590     11.6  \n",
       "1591     11.6  \n",
       "1592     11.0  \n",
       "1593      9.5  \n",
       "1594     10.5  \n",
       "1595     11.2  \n",
       "1596     11.0  \n",
       "1597     10.2  \n",
       "1598     11.0  \n",
       "\n",
       "[1599 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        bad\n",
       "1        bad\n",
       "2        bad\n",
       "3        bad\n",
       "4        bad\n",
       "5        bad\n",
       "6        bad\n",
       "7       good\n",
       "8       good\n",
       "9        bad\n",
       "10       bad\n",
       "11       bad\n",
       "12       bad\n",
       "13       bad\n",
       "14       bad\n",
       "15       bad\n",
       "16      good\n",
       "17       bad\n",
       "18       bad\n",
       "19       bad\n",
       "20       bad\n",
       "21       bad\n",
       "22       bad\n",
       "23       bad\n",
       "24       bad\n",
       "25       bad\n",
       "26       bad\n",
       "27       bad\n",
       "28       bad\n",
       "29       bad\n",
       "        ... \n",
       "1569     bad\n",
       "1570     bad\n",
       "1571     bad\n",
       "1572     bad\n",
       "1573     bad\n",
       "1574     bad\n",
       "1575     bad\n",
       "1576     bad\n",
       "1577     bad\n",
       "1578     bad\n",
       "1579     bad\n",
       "1580     bad\n",
       "1581     bad\n",
       "1582     bad\n",
       "1583     bad\n",
       "1584    good\n",
       "1585     bad\n",
       "1586     bad\n",
       "1587     bad\n",
       "1588     bad\n",
       "1589     bad\n",
       "1590     bad\n",
       "1591     bad\n",
       "1592     bad\n",
       "1593     bad\n",
       "1594     bad\n",
       "1595     bad\n",
       "1596     bad\n",
       "1597     bad\n",
       "1598     bad\n",
       "Name: quality, Length: 1599, dtype: category\n",
       "Categories (2, object): [bad < good]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=2,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "classifier = SVC(kernel = \"rbf\",random_state = 2)\n",
    "classifier.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[355,  45],\n",
       "       [  0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SVC in module sklearn.svm.classes object:\n",
      "\n",
      "class SVC(sklearn.svm.base.BaseSVC)\n",
      " |  SVC(C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      " |  \n",
      " |  C-Support Vector Classification.\n",
      " |  \n",
      " |  The implementation is based on libsvm. The fit time complexity\n",
      " |  is more than quadratic with the number of samples which makes it hard\n",
      " |  to scale to dataset with more than a couple of 10000 samples.\n",
      " |  \n",
      " |  The multiclass support is handled according to a one-vs-one scheme.\n",
      " |  \n",
      " |  For details on the precise mathematical formulation of the provided\n",
      " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      " |  other, see the corresponding section in the narrative documentation:\n",
      " |  :ref:`svm_kernels`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  kernel : string, optional (default='rbf')\n",
      " |      Specifies the kernel type to be used in the algorithm.\n",
      " |      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      " |      a callable.\n",
      " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
      " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |      should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, optional (default=3)\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : float, optional (default='auto')\n",
      " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |      Current default is 'auto' which uses 1 / n_features,\n",
      " |      if ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var())\n",
      " |      as value of gamma. The current default of gamma, 'auto', will change\n",
      " |      to 'scale' in version 0.22. 'auto_deprecated', a deprecated version of\n",
      " |      'auto' is used as a default indicating that no explicit value of gamma\n",
      " |      was passed.\n",
      " |  \n",
      " |  coef0 : float, optional (default=0.0)\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  shrinking : boolean, optional (default=True)\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |  \n",
      " |  probability : boolean, optional (default=False)\n",
      " |      Whether to enable probability estimates. This must be enabled prior\n",
      " |      to calling `fit`, and will slow down that method.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-3)\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  cache_size : float, optional\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to class_weight[i]*C for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : bool, default: False\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, optional (default=-1)\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  decision_function_shape : 'ovo', 'ovr', default='ovr'\n",
      " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      " |      (n_samples, n_classes) as all other classifiers, or the original\n",
      " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      " |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      " |      ('ovo') is always used as multi-class strategy.\n",
      " |  \n",
      " |      .. versionchanged:: 0.19\n",
      " |          decision_function_shape is 'ovr' by default.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *decision_function_shape='ovr'* is recommended.\n",
      " |  \n",
      " |      .. versionchanged:: 0.17\n",
      " |         Deprecated *decision_function_shape='ovo' and None*.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator used when shuffling\n",
      " |      the data for probability estimates. If int, random_state is the\n",
      " |      seed used by the random number generator; If RandomState instance,\n",
      " |      random_state is the random number generator; If None, the random\n",
      " |      number generator is the RandomState instance used by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  support_ : array-like, shape = [n_SV]\n",
      " |      Indices of support vectors.\n",
      " |  \n",
      " |  support_vectors_ : array-like, shape = [n_SV, n_features]\n",
      " |      Support vectors.\n",
      " |  \n",
      " |  n_support_ : array-like, dtype=int32, shape = [n_class]\n",
      " |      Number of support vectors for each class.\n",
      " |  \n",
      " |  dual_coef_ : array, shape = [n_class-1, n_SV]\n",
      " |      Coefficients of the support vector in the decision function.\n",
      " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      " |      The layout of the coefficients in the multiclass case is somewhat\n",
      " |      non-trivial. See the section about multi-class classification in the\n",
      " |      SVM section of the User Guide for details.\n",
      " |  \n",
      " |  coef_ : array, shape = [n_class * (n_class-1) / 2, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  intercept_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  fit_status_ : int\n",
      " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      " |  \n",
      " |  probA_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |  probB_ : array, shape = [n_class * (n_class-1) / 2]\n",
      " |      If probability=True, the parameters learned in Platt scaling to\n",
      " |      produce probability estimates from decision values. If\n",
      " |      probability=False, an empty array. Platt scaling uses the logistic\n",
      " |      function\n",
      " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      " |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      " |      more information on the multiclass case and training procedure see\n",
      " |      section 8 of [1]_.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> y = np.array([1, 1, 2, 2])\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> clf = SVC(gamma='auto')\n",
      " |  >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n",
      " |  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      " |      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      " |      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      " |      tol=0.001, verbose=False)\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVR\n",
      " |      Support Vector Machine for Regression implemented using libsvm.\n",
      " |  \n",
      " |  LinearSVC\n",
      " |      Scalable Linear Support Vector Machine for classification\n",
      " |      implemented using liblinear. Check the See also section of\n",
      " |      LinearSVC for more comparison element.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      " |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      " |  \n",
      " |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      " |      machines and comparison to regularizedlikelihood methods.\"\n",
      " |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SVC\n",
      " |      sklearn.svm.base.BaseSVC\n",
      " |      abc.NewBase\n",
      " |      sklearn.svm.base.BaseLibSVM\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, C=1.0, kernel='rbf', degree=3, gamma='auto_deprecated', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Evaluates the decision function for the samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Returns the decision function of the sample for each class\n",
      " |          in the model.\n",
      " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      " |          n_classes).\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      If decision_function_shape='ovo', the function values are proportional\n",
      " |      to the distance of the samples X to the separating hyperplane. If the\n",
      " |      exact distances are required, divide the function values by the norm of\n",
      " |      the weight vector (``coef_``). See also `this question\n",
      " |      <https://stats.stackexchange.com/questions/14876/\n",
      " |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on samples in X.\n",
      " |      \n",
      " |      For an one-class model, +1 or -1 is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : array, shape (n_samples,)\n",
      " |          Class labels for samples in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseSVC:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Compute log probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the log-probabilities of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          [n_samples_test, n_samples_train]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute `classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples, n_samples).\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target values (class labels in classification, real numbers in\n",
      " |          regression)\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,)\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights\n",
      " |          force the classifier to put more emphasis on these points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      " |      \n",
      " |      If X is a dense array, then the other methods will not support sparse\n",
      " |      matrices as input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.svm.base.BaseLibSVM:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
